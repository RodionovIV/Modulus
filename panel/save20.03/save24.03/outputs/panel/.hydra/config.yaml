training:
  max_steps: 2000000
  grad_agg_freq: 1
  rec_results_freq: 2000
  rec_validation_freq: ${training.rec_results_freq}
  rec_inference_freq: ${training.rec_results_freq}
  rec_monitor_freq: ${training.rec_results_freq}
  rec_constraint_freq: 10000
  save_network_freq: 1000
  print_stats_freq: 100
  summary_freq: 1000
  amp: false
  amp_dtype: float16
  ntk:
    use_ntk: false
    save_name: null
    run_freq: 1000
profiler:
  profile: false
  start_step: 0
  end_step: 100
  name: nvtx
network_dir: .
initialization_network_dir: ''
save_filetypes: vtk
summary_histograms: false
jit: true
device: ''
debug: false
run_mode: train
arch:
  fully_connected:
    _target_: modulus.architecture.fully_connected.FullyConnectedArch
    layer_size: 1024
    nr_layers: 6
    skip_connections: false
    adaptive_activations: false
    weight_norm: true
loss:
  _target_: modulus.aggregator.Sum
  weights: null
optimizer:
  _params_:
    compute_gradients: adam_compute_gradients
    apply_gradients: adam_apply_gradients
  _target_: torch.optim.Adam
  lr: 0.001
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0.0
  amsgrad: false
scheduler:
  _target_: custom
  _name_: tf.ExponentialLR
  decay_rate: 0.95
  decay_steps: 15000
batch_size:
  panel_left: 250
  panel_right: 250
  panel_bottom: 150
  panel_corner: 5
  panel_top: 150
  panel_window: 3500
  lr_interior: 7000
  hr_interior: 4000
custom: ???
